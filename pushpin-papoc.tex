\documentclass[sigplan,10pt]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{todonotes}

\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmlicensed}
\acmConference[PaPoC '20]{7th Workshop on Principles and Practice of Consistency for Distributed Data}{April 27, 2020}{Heraklion, Greece}
\acmBooktitle{7th Workshop on Principles and Practice of Consistency for Distributed Data (PaPoC '20), April 27, 2020, Heraklion, Greece}
\acmDOI{}
\acmPrice{}
\acmISBN{}

\hyphenation{Web-RTC}

\begin{document}
\title{PushPin: Towards Production-Quality  Peer-to-Peer Collaboration}

\author{Peter van Hardenberg}
\email{pvh@inkandswitch.com}
\affiliation{%
  \institution{Ink \& Switch, LLC}
  \city{San Francisco}
  \state{CA}
  \postcode{}
  \country{USA}
}

\author{Martin Kleppmann}
\email{mk428@cl.cam.ac.uk}
\orcid{0000-0001-7252-6958}
\affiliation{%
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \state{}
  \postcode{CB3 0FD}
  \country{United Kingdom}
}

\begin{abstract}
Fully peer-to-peer application software promises many benefits over cloud software, in particular, being able to function indefinitely without requiring servers.
Research on distributed consistency mechanisms such as CRDTs has laid the foundation for P2P data synchronisation and collaboration.
In this paper we report on our experience in taking these technologies beyond research prototypes, and working towards commercial-grade P2P collaboration software.
We identify approaches that work well in our experience, such as the functional reactive programming paradigm, and highlight areas in need of further research, such as the reliability of NAT traversal and usability challenges.
\end{abstract}

\begin{CCSXML}
<ccs2012>
    <concept>
        <concept_id>10003033.10003039.10003051.10003052</concept_id>
        <concept_desc>Networks~Peer-to-peer protocols</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10011007.10010940.10010971.10010972.10010540</concept_id>
        <concept_desc>Software and its engineering~Peer-to-peer architectures</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10003120.10003130.10003233</concept_id>
        <concept_desc>Human-centered computing~Collaborative and social computing systems and tools</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10011007.10010940.10010992.10010993.10010961</concept_id>
        <concept_desc>Software and its engineering~Synchronization</concept_desc>
        <concept_significance>300</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Networks~Peer-to-peer protocols}
\ccsdesc[500]{Software and its engineering~Peer-to-peer architectures}
\ccsdesc[500]{Human-centered computing~Collaborative and social computing systems and tools}
\ccsdesc[300]{Software and its engineering~Synchronization}

\keywords{real-time collaboration, CRDTs, peer-to-peer protocols, distributed programming, usability}

\maketitle

\section{Introduction}

In the past, software used to run on one computer and store its data on the local disk.
Now, increasingly, we expect our software and data to be available on multiple devices, with synchronisation across devices belonging to the same user (e.g.\ laptop, smartphone, tablet), and also allowing real-time collaboration between multiple users.

The standard way of implementing such multi-user, multi-device software today is to rely on cloud services that store the authoritative copy of the users' data, and which can be accessed through thin clients such as web browsers and mobile apps.
However, reliance on the cloud comes with problems: services require ongoing maintenance by expensive 24/7 operations teams, and they cease to exist when the organisation backing them terminates their funding.
If a cloud service shuts down, users lose access to all data stored in that service, unless some migration path to an alternative service is provided.
Moreover, cloud-centric software often does not work well offline, and can be slow as the client waits for round-trips to the server in order to load or store data there.

Peer-to-peer application software promises to overcome these problems.
In principle, a P2P system should be able to function indefinitely, without depending on someone paying the server bills to keep a cloud service running.
Storing an authoritative copy of data on the users' devices enables offline work and stronger data ownership~\cite{LocalFirst}, and synchronising updates through a P2P network should allow the same kind of real-time collaboration that we know from cloud software.

While various research prototypes of P2P collaboration software have been developed, we are yet to see any mainstream applications using this approach.
Technologies such as Conflict-free Replicated Data Types (CRDTs)~\cite{Shapiro:2011un} and P2P data replication protocols (see Section~\ref{sec:networking}) provide a good foundation for this type of software, but there is still a big gap between these building blocks and the requirements of software with the features expected by users today.

For example, there are at least half a dozen CRDT algorithms for P2P collaboration on a plain text document~\cite{Kleppmann:2019iu}, and various P2P protocols that can replicate a static file.
However, there is little existing work on how to use these building blocks to implement a rich workspace containing potentially thousands of documents and files of various types, with facilities for organising, searching, and selectively sharing this content with other users.

The PushPin project is investigating if and how we can develop commercial-quality collaboration software with minimal reliance on servers.
Our goal is not to create new algorithms or protocols, but rather to evaluate existing technologies by developing an example P2P application with a mindset of industrial software development best practices and realistic user requirements.
We approach this project with broad interests in exploring programming models, P2P data distribution, reliability, and usability.

In summary, our findings are:
\begin{itemize}
    \item CRDTs provide a reliable, principled foundation for P2P collaboration.
    We found the details of conflict resolution semantics to be surprisingly unimportant, as conflicts seem to occur rarely in practice.
    \item The Functional Reactive Programming (FRP) approach is a good way of creating user interfaces on top of state managed by CRDTs (Section~\ref{sec:data-model-ui}).
    \item P2P protocols are poorly supported by many network routers (Section~\ref{sec:networking}).
    Consequently, the use of servers to forward network traffic is sometimes unavoidable.
    \item The performance of CRDTs and P2P replication implementations becomes problematic as the amount of data grows. The storage and processing overheads of current implementations are substantial.
    \item Significant open problems remain, including around authentication and access control, indexing and search, schema evolution and compatibility, privacy, and the usability of systems where some devices are in sync while others are not.
\end{itemize}

% The prize for delivering a truly peer-to-peer application development system is great.
% centralised cloud-based software has created the expectation of universal access to both canonical versions of software and to users' data from any internet-connected computer at any moment, with full collaborative functionality built in.

%To this end, we must reconsider our relationship with vast centralised databases and explore new methodologies for building software.

% document CRDTs
% functional reactive programming
% browser rendering
% "document oriented programming"

\section{Design Principles}\label{sec:principles}

Before going into the details of the implementation of PushPin we outline the principles we applied to its design and development.

\subsection{Local-first software}

While we want users' data to be accessible on multiple devices, the authoritative copies of this data should reside on the users' local computers, not in the cloud.
If servers are used, these replicas should be considered secondary copies that exist only to facilitate data synchronisation and backup, but they should not be considered authoritative.
In previous work we have coined the term \emph{local-first software} to describe software that adheres to this principle~\cite{LocalFirst}.

With a local-first approach, the software continues working fully if the user's computer is disconnected from the Internet: cross-device synchronisation happens in the background when a network connection is available.
Even if all servers are shut down, the copy of the data on the local disk remains fully functional and under the user's control.
The user can manage this data like any other local files, e.g.\ copying it, backing it up, or converting it into another format.

\subsection{Minimal dependence on servers}

We want the software to continue working indefinitely, without being vulnerable to outages if the organisation running some piece of infrastructure goes out of business.
Thus, in addition to local data storage on each device, the cross-device data synchronisation mechanism should also depend on servers to the least degree possible.

In some environments it is possible to operate entirely without servers, while in other cases a small amount of centralised infrastructure seems to be inevitable with current technologies.
However, if servers are used, we want them to be as simple, generic, and fungible as possible, so that one unavailable server can easily be replaced by another.

\subsection{Conflict-free data synchronisation}

The combination of using local on-device storage, support for offline editing, and synchronisation without servers implies that our application is \emph{intrinsically distributed}.
Each device serves as a replica, and it would not make sense to enforce any kind of single system image semantics across replicas, since that would imply that a device must wait for synchronous coordination with other devices whenever any data is changed.
We cannot rely on consensus algorithms, which must wait for communication with a quorum of replicas.

Rather, we have to accept that each device has its own local view onto the shared data, and that those views may diverge as users update their data.
As devices exchange updates, they converge again by merging their states.
We do this using conflict-free replicated data types (CRDTs)~\cite{Shapiro:2011un}.

\subsection{Mainstream, as far as possible}

In order to explore the \emph{user experience} implications of a peer-to-peer architecture, we wanted to develop not just a rough research prototype, but polished end-user software that is on par with commercial applications available today, with a thoughtful graphical and interaction design.
We also wanted to explore the \emph{developer experience} of peer-to-peer software, to understand how writing software with this architecture could become accessible to mainstream software engineers.
Thus, we wanted to base our work on mainstream languages and platforms as far as possible.

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{pushpin.jpg}
    \caption{Screenshot of PushPin. The main user interface consists of cards of various types (text, image, PDF, \dots) that can be freely arranged on a 2D ``board''. Boards can be nested within other boards. The toolbar at the top provides navigation between boards and sharing settings.}
    \label{fig:pushpin}
\end{figure*}

\section{PushPin: A Collaborative Corkboard}\label{sec:pushpin}

The PushPin software~\cite{PushPinSource}, shown in Figure~\ref{fig:pushpin}, allows users to collect media of various types (including text, web pages, images, and PDF files), to archive and organise it.
Media files are visually represented as \emph{cards} on an infinite two-dimensional \emph{board}, where they can be resized and positioned arbitrarily.
One board may be nested within another board, enabling hierarchical organisation and navigation.
This board metaphor is inspired by software such as Miro~\cite{Miro} and Milanote~\cite{Milanote}.

We chose this application because it fits well with the principles articulated in Section~\ref{sec:principles}: the data in this software belongs to the user, and there are essentially no restrictions as to what the user may do with their data.
Unlike some systems (e.g.\ banking or payment systems, auction websites, ride-sharing services, or games), in this application there is no need to enforce any global rules or consensus across users, and there is no need for an authority to decide what actions are allowed.
The only restrictions are access permissions (i.e.\ defining which user may view or modify which pieces of data), but we assume that a user with permission to edit some piece of data may modify it in any way.


\subsection{Building desktop software with Electron}

In recent years there has been considerable innovation in web application technologies, including in web browsers (new features of HTML and CSS), languages (e.g.\ TypeScript), user interface libraries (especially React~\cite{React}), and JavaScript modules for a wide variety of tasks (e.g.\ PDF rendering~\cite{PDFjs}).
In order to take advantage of this lively ecosystem, we decided to implement PushPin using web technologies.

However, web applications running in a browser tab have constraints that make them unsuitable for local-first/P2P use. These constraints fall in two categories:
\begin{description}
\item[Storage.]
By default, browsers do not reliably store data.
Web applications can use localStorage, IndexedDB or Progressive Web App features to store data on disk, but this data is silently deleted when a user chooses to clear cookies in their browser~\cite{LocalStorageCleared}, and browser caches can expire data without notification.
Users have no way of predicting whether their data or the application will be available offline.

Sometimes, manual steps are required: for example, Google Docs requires installing a special browser extension to enable offline support, and it only makes documents available offline when they are specifically selected by the user. Anecdotally, it appears to be common for users to think they had enabled offline usage, only to realise later that their data is unavailable.

\item[Networking.]
For security reasons, web browsers restrict the network communication that application code can perform: client-server requests are restricted to HTTP or WebSocket, and are subject to the same-origin policy \cite{SameOrigin}. Peer-to-peer communication is restricted to WebRTC (see Section~\ref{sec:existing-p2p}).
It is not possible to use arbitrary TCP or UDP networking, which would be required to implement other peer-to-peer protocols.
\end{description}

We avoid these limitations by building on Electron~\cite{Electron}.
Electron runs a JavaScript web application in a dedicated Chromium-based browser runtime, packaged as a downloadable and locally installed executable containing all of the required code.
Once installed, the user can be sure that the application is available offline.

Electron also makes Node.js APIs available to application code, which enables full access to the local filesystem, and socket APIs allowing arbitrary TCP and UDP networking.
Thus, Electron allows software engineers with web development skills to write cross-platform desktop software for macOS, Windows, and Linux.

\begin{figure*}
\begin{verbatim}
{
  "title": "Inspiration",
  "authors": [
    "hypermerge:/BvRmN2rU7pQBginzw9KqTXcUmtyYC5aCLiZZ314Ga8Vt?contentType=contact",
    "hypermerge:/6YTvUkNePGkPJH3fHaCoJfJdVdKbi6qjDKgp27u3SChG?contentType=contact"
  ],
  "cards": [
    { "x": 0, "y": 0, "w": 577, "h": 484,
      "url": "hypermerge:/4uhU1SDgy56cAo3dQH5tqSrrTAZnQSQKkEUPzNUXSvcV?contentType=image" },
    { "x": 0, "y": 486, "w": 577, "h": 95,
      "url": "hypermerge:/HHmxeCceWD1ZBrXKeHPv7k7umVW2ncsWyVrUiV3cmMYP?contentType=text" },
    { "x": 612, "y": 84, "w": 370, "h": 862,
      "url": "hypermerge:/E6qRRVUsbRcjddCV98TLyyRhPsVNgyiUXAdPsLyKM3mn?contentType=todo-list" }
  ]
}
\end{verbatim}
\caption{JSON document representing a PushPin board with three cards on it.}
\label{fig:board-json}
\end{figure*}


\subsection{Automerge Documents}

All application state in PushPin is managed by Automerge~\cite{Automerge:2018,Automerge}, a JavaScript CRDT library that provides a JSON data model~\cite{Kleppmann:2017ca}.
Automerge defines a format in which data updates can be written to local disk and replicated to instances of PushPin on other devices.
Network communication is provided by a separate layer called Hypermerge, which we discuss in Section~\ref{sec:networking}.

An Automerge document is the unit of replication and sharing in PushPin: that is, a user can access either all or none of a document.
We want a PushPin user to be able to share their content with other users at a fine-grained level: e.g.\ one card at a time, or one board at a time.
For this reason, we represent each card and each board as a separate document.

For example, Figure~\ref{fig:board-json} shows the JSON representation of a board.
It contains a \texttt{title} (rendered in the toolbar in Figure~\ref{fig:pushpin}), a list of users who have contributed to the board, and a list of cards on the board.
Each card has \texttt{x} and \texttt{y} attributes recording its position, \texttt{w} and \texttt{h} attributes for width and height, and a \texttt{url} linking to its content (in a separate document).

Automerge allows state to be concurrently updated on different devices, and it ensures that replicas converge to the same state as they communicate.
For example, if a user drags a card to a new position, this state change is modelled by updating the \texttt{x} and \texttt{y} attributes of that card, while the rest of the document remains unchanged.
If another user concurrently creates a new card on another device, it is inserted into the list of cards.
The CRDT tracks that change and records it as an Automerge operation for replication.
The concurrent updates (changing the position of one card and adding another card) are merged cleanly on each replica.

\subsection{URLs and Linking}

Each document is identified by a unique \texttt{hypermerge} URL of the form shown in Figure~\ref{fig:board-json}.
Given a URL, a PushPin instance can obtain the corresponding document content through a process described in Section~\ref{sec:peer-discovery}.
By including one document's URL in the content of another document, we form a graph of links, similar to the web.

The same URL can be referenced from multiple places, allowing e.g.\ the same card to be embedded on multiple boards.
URLs can also be shared with another user by sending them through any communication channel, such as email.
When a PushPin instance loads a document containing a \texttt{hypermerge} URL, it eagerly resolves and downloads the content belonging to that URL.
Thus, any transitively reachable documents are automatically added to PushPin's local document storage on that device, making them available offline.

Each URL also includes a \texttt{contentType} parameter, which indicates how that document should be rendered in the user interface.
This parameter is part of the URL, not the document content, because the same document content may be rendered differently in different contexts.
For example, PushPin could be extended to support flashcards for language learning.
In one context, the document containing the database of flashcards could be rendered as a list of entries, while in another context it might be rendered as a quiz interface, presenting one side of one card at a time.

\begin{figure*}
\caption{The data flow of Document Functional Reactive Programming (DFRP)}
\label{fig:document-frp}
\end{figure*}

\section{Creating User Interfaces for CRDTs}\label{sec:data-model-ui}

In traditional, server-centric web applications, synchronisation between client-side and server-side state is usually performed in an ad-hoc way, with user actions translating into HTTP requests that perform API calls on a server.
In contrast, CRDTs provide us with a principled way of managing and reasoning about the state of multiple replicas: each replica can optimistically update its local state, and a synchronisation protocol running in the background ensures that those replicas eventually converge~\cite{Saito:2005jw}.

However, the convergence we have discussed so far is at the level of Automerge documents, such as the JSON shown in Figure~\ref{fig:board-json}.
In this section we will expand the discussion to include the state of the user interface.

\subsection{Functional Reactive Programming (FRP)}

For PushPin we wanted a similarly principled approach to developing user interfaces, which would allow the state of the CRDT and the state of the user interface to be kept in sync in a way that is robust and easy to reason about.

We solved this problem using the Functional Reactive Programming (FRP) approach~\cite{Czaplicki:2013ig}, which has been gaining popularity in JavaScript web applications, especially due to its implementation in Facebook's React library~\cite{React}.

FRP works by defining a deterministic \emph{render} function that takes the current state of an application, and returns a description of the user interface reflecting that state.
Whenever the user performs an action, the action results in an updated state.
The updated state is then passed to the render function in order to determine the updated user interface.
In practice, performance is improved by detecting which parts of the state are unchanged, and rendering and updating only those parts that have changed.
However, the conceptual simplicity remains: the render function cleanly translates between application state and user interface state.

\subsection{Document FRP (DFRP)}

In the usual implementation of FRP, the application state (the input to the render function) is simply an in-memory object.
In PushPin, we generalise this approach by making an Automerge document the input to the render function.
We call this approach \emph{Document FRP} (DFRP).

Figure~\ref{fig:document-frp} illustrates the DFRP data flow.
An Automerge document may be newly created, or loaded from local disk, or fetched over the network.
Regardless of its origin, the document can be displayed by passing it to the deterministic render function.

The resulting user interface has attached event handlers that detect user input, such as mouse clicks or keyboard input.
When any such events occur, the Automerge document is updated to reflect the user input, and the render function is called again to refresh the user interface.
This local interaction can be performed regardless of whether the user is currently connected to the Internet.

Likewise, whenever the network synchronisation protocol running in the background receives an update from another replica, Automerge applies this update to the CRDT state.
We then again call the render function with this updated state, which refreshes the user interface accordingly.
Thus, the user interface logic makes no distinction between a local user's updates and remote updates received over the network: both simply result in a call to the render function.

In applications written in the DFRP style, the developer never needs to worry about API calls to a server backend, including the fact that they may fail.
Instead, all communication is via shared CRDT state that can be updated by any collaborator at any time.
This means DFRP software can actually be \emph{simpler} to develop than traditional web applications, even though it is also more powerful.

% challenges: getting into weird states, write permissions

\subsection{Ephemeral state, local-only and shared}

Our earliest experiments put all data into the CRDT. This quickly proved to be problematic, as many useful data like unsubmitted text in HTML textboxes or cursor positions are ill-suited to universal consensus. Also, some data, like cursor position or timers for tracking animations, can completely swamp the CRDT with low-value information.

The first natural extension then was to redefine the function describing the application state to include a second ephemeral local-only state that is neither persisted nor distributed. Some of this kind of ephemeral or high-velocity data is, however, useful to share with online peers.

It can be helpful to see whether other users are viewing the same document as you, or what text or items they have selected. It might be interesting to see another user's current position in a podcast or video, or a hint that shows they're currently typing into a chat textbox they've not yet submitted.

For this data PushPin uses a special messaging channel adjacent to the CRDT that ties arbitrary messages to a device and user context. The current implementation is rudimentary. The ephemeral data is not associated with a particular CRDT state, and is distributed only over live connections so will not function as expected in complex network topologies or at large scale. Still, it has allowed us to add important liveliness to the user experience of PushPin. Particularly in distributed applications it is important to communicate the feeling of presence when other users are online or collaborating, because unlike in a centralized system there are many ways to be both online or off.

\subsection{Multiple Documents}

Decomposing the view into a graph of documents allows for significant simplification of the overall FRP update functions and makes them reusable. The same code that renders a text card, or a user avatar can render that document in some other context. Indeed, it is possible to create generic, reusable renderers that visualise arbitrary documents, or specialised renderers that render the same document in new contexts. A simple example of this is the PushPin title-bar. The current document is re-rendered by a second FRP function that only renders the title field and list of authors. 


\subsection{Notes on CRDT Performance}

Automerge's performance has proven adequate for our purposes, but we believe that "next-frame" performance is essential for high-quality user experience. This implies that the application's front-end should never be blocked by performing background tasks like encryption, CRDT computation, or other non-essential work. Many web applications use a technique called "optimistic UI updates" to allow for local changes to be represented in the front-end before they are confirmed elsewhere. Pushpin implements this holistically across all documents in the application by separating the front-end, a mechanically simple interface to fully assembled CRDT documents, from the back-end, where CRDTs are compiled out of the streams of operations that describe them. When a change is made in the front-end, it is always applied there immediately and also submitted to the back-end, which runs in another process. The back-end applies the change, integrating it with any other waiting changes, and forward the resulting finalised document back to the front-end.

This technique, which we have not seen realised in other systems, allows us to reliably deliver a 60 frames-per-second user experience even when there are many CRDT operations ongoing in a busy collaboration.

\subsection{Cloud Peers for Storage, Enhanced Connectivity \& Other Uses}

\section{Peer-to-peer Networking for Collaboration}\label{sec:networking}

Every networked application relies on three core facilities provided by the networking stack: discovering the network address to connect to, establishing a connection, and securing the confidentiality and integrity of the data transfer.

In a traditional web application, discovery is provided by DNS, connection by TCP, and security by SSL/TLS.
However, these technologies are not a good fit with our goal of minimising centralised infrastructure and ongoing cost:
\begin{itemize}
    \item DNS requires paying ongoing registration fees for domain names, and it requires running DNS servers.
    \item TCP requires the server to have a publicly routeable IP address; it cannot connect directly to most end-user devices as they are behind NAT (see Section~\ref{sec:nat-traversal}).
    \item SSL/TLS certificates are tied to domain names, which incur registration fees.
\end{itemize}

The peer-to-peer technologies we explored in PushPin attempt to overcome the need for centralised infrastructure.

\subsection{Existing Peer-to-Peer Technologies}\label{sec:existing-p2p}

We considered several P2P networking stacks for PushPin:
\begin{description}
\item[WebRTC] is a peer-to-peer protocol built into modern web browsers.
It is primarily designed for audio and video calls, but it can also carry application data.
WebRTC does not provide a peer discovery mechanism; typically, applications rely on a server to help peers discover each others' IP addresses (this process is called \emph{signaling}).
\item[BitTorrent] is widely used for file sharing.
It provides a distributed hash table (DHT) for peer discovery, and uses the uTP protocol to establish connections between peers.
However, it is designed for static files, and is not suitable for data that is constantly changing, like in collaboration software.
\item[IPFS] aims to provide decentralised storage through a networking stack called \emph{libp2p}.
Like BitTorrent, it is mostly focused on replicating static files; it provides limited support for changing data through its IPNS and PubSub modules, but these features are immature at the time of writing.
\item[Dat] \cite{HowDatWorks,Ogden:2018ur} is a peer-to-peer data sharing platform.
For peer discovery it uses a centralised DNS service hosted by a nonprofit foundation (a distributed hash table is under development), and it uses BitTorrent's uTP protocol to establish connections.
\end{description}

PushPin builds upon the \emph{hypercore} protocol and implementation from the Dat project~\cite{HowDatWorks}, since its focus on replicating mutable data makes it the best fit for our needs.

A hypercore is an append-only log that is authenticated with a public key; only the owner of the corresponding private key can modify the log, but many peers can store replicas of the log.
We map each PushPin document to a set of hypercores, with one hypercore per device that has edited the document.

\subsection{Peer Discovery}\label{sec:peer-discovery}

The Dat protocol allows the replicas of a hypercore to be discovered based on a hash of its public key~\cite{HowDatWorks}.
We encode this public key in the form of a URL; thus, the URL is a stable identifier for the document, even as its content changes.
Knowledge of the URL allows a peer to obtain a copy of the document, via the peer discovery mechanism.
We allow one document to reference another by including the referenced document's URL in another document.

When peers are on the same LAN (wired or wireless network), they attempt to discover one another using mDNS, a variation on DNS that broadcasts DNS-like service advertisements or discovery requests to a well known multicast IP address.
If successful, the peers can connect directly via TCP and begin exchanging data.
This mode of discovery is appealing since it depends only on the local network: communication between peers does not flow via the Internet, and it does not depend on any centralised infrastructure.

When peers are not on the same LAN, the Dat protocol uses a centralised DNS server for peer discovery.
This approach is not fully peer-to-peer, but at present this seems to be a necessary compromise to make.
Since the DNS server only provides peer discovery, and does not handle any data transfer traffic, it is quite cheap to run.

\subsection{NAT Traversal}\label{sec:nat-traversal}

Due to a shortage of IPv4 addresses, most personal computing devices do not have a globally reachable IP address, but rather a local address in a reserved space (e.g.\ 192.168.x.x or 10.x.x.x).
When such a device wishes to establish a connection to another, the local router records the destination of outbound traffic and routes responses back to the originating local client.
This process is called Network Address Translation (NAT).

A device behind NAT can make outbound connections, but it cannot receive inbound TCP connections from outside of the NAT.
An exception: in home environments, where the user has control over their own router, the UPnP standard allows devices to reserve particular ports on the router's public IP address as the destination for inbound connections.
However, mobile devices often use networks with NAT on which UPnP is not available, such as a coffee shop WiFi, a corporate office network, or a cellular data network.

In these cases, the most viable solution is known as ``hole punching'' or NAT traversal \cite{RFC5389}.
This process requires the temporary intervention of a third host to introduce the two peers, and both peers sending UDP packets to each other, allowing a connection to be established.
NAT traversal is performed by BitTorrent's uTP protocol, and by the STUN protocol in WebRTC \cite{RFC5389}.

However, there are situations in which neither the LAN discovery approach nor NAT traversal works.
For example, some coffee-shop WiFi and some corporate networks are set up in a ``guest network'' mode, which prevents all local connections between devices on the network (intended as a security measure to prevent inadvertent sharing of data with other users on a public network).
Without local traffic, we attempt to fall back on NAT traversal; however, this approach also fails, since many routers in their default configuration refuse to create NAT traversing routes that originate and terminate within the same network.

In this case, establishing a direct connection between the peers seems to be impossible, and the only remaining option is to use a server to proxy the communication between the peers, e.g.\ using the TURN protocol~\cite{RFC5766}.
In the case of PushPin, users can either run their own publicly addressable network hosts or take advantage of a TURN server provided by the community.

\subsection{Storage Peers}

A limitation of any peer-to-peer system is that two peers can only communicate while they are both online.
However, mobile devices are often offline, making it difficult to find an opportunity to synchronise.
For example, if your colleague has shared the URL of a PushPin board with you, it would be annoying if you could not access that board because the only copy of the board resides on your colleague's laptop, and that laptop's lid is currently closed.

We can overcome this limitation by introducing \emph{storage peers}, which replicate all the data belonging to a particular user or set of users.
A storage peer runs the same replication protocol as any other peer; the difference is only that it is always online, allowing other devices to sync with the storage peer at any time.
Storage peers also provide a form of backup.

Unlike a traditional server, a storage peer can be reached through NAT traversal, so it does not need a public IP address: for example, it could be a device on the user's home internet connection.
Since it only stores the data for a small number of users, it does not need to be a powerful machine.
We have experimented with using a Raspberry Pi as storage peer, writing data to an SD card.

\todo[inline]{Here's more text on cloud peers, merge it with the text above}

As discussed elsewhere, one of our key goals with this project was to make cloud infrastructure optional rather than mandatory, but we recognize that servers have an important role to play in any resilient system. For PushPin, we implemented pushpin-peer, a sibling project that runs the same document sharing stack but as a simple Unix daemon without any visual front-end. The main role for pushpin-peer is data presence. A user can store a URL with pushpin-peer and the tool will download all subsequent versions of that document, spider out through any links within that document (and so on from those), and of course, it can serve up that data to other clients requesting it. We feel this is an important distinction from the notion of "pinning" found in some other systems such as IPFS. In general, PushPin is a creative tool oriented towards storing data created locally by the user or by one of their collaborators. The default should be to preserve everything! In contrast, IPFS is oriented towards generic binary object storage, and so does not assume that interacting with an object would imply wanting to retain it indefinitely.

One of the goals for pushpin-peer was that the UI had to be self-hosted, which is to say that all the interactions with it should occur within PushPin and not in some other web administration tool. When pushpin-peer starts up, it prints out a URL that users can load from their PushPin. Users interact with their cloud peer from within PushPin by making edits to PushPin's adminstration document. The pushpin-peer daemon monitors that document for changes and takes actions based on what it finds. This approach was first pioneered in our work on pixelpusher, and has returned with greater sophistication here. Most importantly, pushpin-peer uses similar techniques to the user document sharing approach described above to allow clients to write private URLs to a document other peers may read without disclosing their contents.

\subsection{Data Confidentiality and Integrity}

Some peer-to-peer systems, such as BitTorrent and IPFS, use a content-addressable storage approach to data integrity: every file is identified by the hash of its contents, and the recipient can check the integrity of the file by comparing its actual hash to the expected hash.
(In fact, BitTorrent uses Merkle trees \cite{Merkle:1987} to allow efficient incremental validation of data during its download.)
However, this approach is not suitable for collaboration software, where the the hash would frequently change as data is modified.
Instead, Dat relies on digital signatures to ensure that nobody can make undetected alterations to the data without knowing the private key.

The Dat protocol encrypts the communication between peers, using the URL of the document as a shared secret to establish the encryption key~\cite{HowDatWorks}.
Thus, the URL acts as a bearer token (or capability) that grants read access to the document to anyone who knows it.
We are interested in stronger access control and end-to-end encryption protocols for collaboration software~\cite{Kleppmann:2018tk}, but we have left this issue out of scope in the PushPin project.

The peer discovery protocol uses hashes of URLs, not the URLs themselves, so that anyone observing the peer discovery traffic (such as the DNS server, or other devices on the local network when using mDNS) does not gain the ability to read the document.

\subsection{Scalability of Peer Discovery}

% (It should be noted that in practice today, the BitTorrent DHT has proven itself operationally resilient but other implementations such as IPFS have struggled to remain useful during flooding attacks.)

On a local network, a Dat peer broadcasts all the hashes of URLs for data it holds, and all the hashes it requests.
This approach is suitable when the number of URLs is small, but it breaks down as collections expand.
Since PushPin uses a separate URL for each card, a user quickly accumulates many hundreds or thousands of document URLs.
In our testing we found that we could fairly reliably crash the consumer-grade WiFi routers found in short-term rentals with half a dozen researchers sharing their collections.

% TODO: diagram of append-only logs, discovery keys, maybe broadcasting?

The Secure Scuttlebutt~\cite{Tarr:2019ba} project avoids this problem by placing all of a user's activity into a single log, which it then merges with all of that user's peers and their peers out for several degrees of social connection. This trades one problem for another: by merging all of a user's data (and their peers' data) into one feed, there is no way to selectively synchronise that user. A peer either downloads all, or none.

Fortunately, we have observed that users tend to collaborate on more than one document with the same collaborator.
Thus, when searching for peers that have a copy of a new document, it is likely that this document of interest can be found in the repository of a peer you are already connected to.
We can significantly reduce the amount of discovery network traffic by first querying existing peers, and only performing a global DHT lookup if this fails.
Perhaps these peers could also forward queries on our behalf, recreating a DHT-like network.
This is an area for future work.

\subsection{Metadata Privacy}

A downside of peer-to-peer protocols is that the peer discovery mechanisms leak information about users to other nodes on the network.
Although the content of documents is only available to peers who know their URLs, the discovery keys (hashes of URLs) are widely broadcast, allowing a user's device to be identified by the pattern of discovery keys it shares.
With this information, an attacker can monitor a user's IP address over time, and thus track their approximate physical location.

A number of defences could reduce this tracking potential (e.g.\ automatic rotation of discovery keys, or some form of interactive proof exchange through a third party prior to exposing IP addresses), but this remains an area of active concern and research.

\todo[inline]{Include this somewhere:}

Unfortunately, the stability of these URLs is somewhat problematically strong. The URLs function as a sort of security capability, and once a client has seen the URL, there is no real mechanism in the current system to revoke or rotate it. Further, while we believe CRDTs are an exciting area to explore new collaboration models \cite{Pixelpusher} we have not pursued this effort in PushPin. We have often described our approach in PushPin as "radical mandatory collaboration". Once a document has been shared, a client will always accept all updates to it from all users with the URL anywhere forever. This is, to be clear, not a good design, but rather a deliberate absence of design. 

\subsection{Online/offline state}

PushPin is designed to support not just collaboration between people, but also collaboration across a users' own devices \footnote{Notably, PushPin does not have mobile support and many elements of its design are unavailable on most mainstream mobile devices today.}. A user document includes a list of devices they are associated with, and users are considered online if any of those devices are reachable. Availability of a users' own devices is also important. A user with no other online devices is making changes without a backup. If another of their devices is online, their work can be synchronised there and become more durable. It can be important to a user to understand their relative state to a variety of users. If PushPin has sent a fresh copy of a document to an offline user just a moment ago, that's meaningfully different than if the user does not appear to have used the application in weeks. 

\section{Lessons from Implementing PushPin}\label{sec:lessons}

Our goal in building PushPin was to explore whether it was possible to apply recent developments in CRDTs and peer-to-peer networking with modern web development technologies to produce an application with several properties.

Those properties are:
 - long-now reliability, by not relying on external network services
 - real-time collaboration throughout the software
 - consistent performance with low latency
 - a high degree of usability for a general audience 

Because of the first and second goals Peer-to-peer systems have a number of unique challenges versus centralised systems, including:

\begin{itemize}
	\item no single source of truth, no single authority

	Without a centralised server hosting an authoritative copy, every copy of the data becomes, in some sense, authoritative. This implies that we must have some kind of robust merging strategy at the core of our design.
	
	\item devices get out of sync

    Because we insist that each device is always able to make progress without communication with other nodes, it is vitally important to both do our best to synchronise our various devices, but also to communicate synchronisation state. What were the last changes received from our laptop? Did the changes we made locally get uploaded to a server before the network connection was lost?  

    \item connectivity is non-boolean

    With a centralised system a user is either offline or online. The central node routes all data. With a peer-to-peer system is is entirely possible that you might be connected with another user around the world but due to the complexities of computer networking, disconnected from a user in the same room. (Perhaps their wifi connection is offline.) 

    \item changes can come from anywhere at any time

    Without the centralised system approving or rejecting changes against a monotonically incrementing API version, it's possible that other clients are running older, newer, or just different versions of the program. How do we robustly manage change over time?
    
    \item different devices might be running different versions of the software, and they need to interoperate cleanly
\end{itemize}

It is a sad truth that peer-to-peer software has earned a rather poor reputation not just for reliability but also as a vector of illicit activity. Because of this, router vendors and network administrators both collude to restrict peer-to-peer communications and simply fail to support them.

what works, what's not working so well?

\begin{itemize}
    \item FRP render loop works great -- taking a functional transform of a document state to application view means never worrying about where the update came from or how to render it
	\item storing \& distributing append-only logs is very simple and robust
    \item peer-to-peer networking is highly problematic
    \begin{itemize}
	    \item webrtc (not very good, requires centralized assets)
		\item DHTs (reliability, privacy)
		\item centralized services (fragility)
		\item router configuration issues
	\end{itemize}
	\item much performance work has been done
	\begin{itemize}
	    \item architect to separate render \& computation of CRDT operations
	\end{itemize}
	\item conflict resolution surprisingly unproblematic
	\item merge \& collaboration UX is largely unexplored (could show a pixelpusher slide \& briefly explain problems)
	\item what kind of identity, privacy, sharing features are important / feasible here?
	\item html is a pretty rough application development platform
	\item no real mobile support
	\begin{itemize}
	    \item what's the cross-platform story?
	\end{itemize}
	\item URLs containing long alphanumeric strings (corresponding to cryptographic keys) are a poor user experience. We would like to minimise the degree to which users have to copy and paste them.
\end{itemize}

By creating documents out of trustworthy append-only logs with stable names, we enable links and stably-named collaborative documents. A variety of state-of-the-art networking strategies overcomes the limitations of commonly available NAT routers in the field. Unfortunately, these techniques are not reliable enough to eliminate all traffic proxy requirements, and so we employ optional, but helpful network-persistent peers which act as relay points and data caches to improve reliability. (Importantly, they are not a central resource but can be run by any motivated network participant.) Last, we discuss some of our early work in improving discovery strategies for scalability without sacrificing specificity and some of the problems with current distributed hash table implementations today.

\section{Conclusions}

Let us begin by reiterating our goals:
\begin{itemize}
    \item To deliver 
\end{itemize}

CRDT / Application Model
\begin{itemize}
	\item We find modelling collaboration sessions as Automerge CRDTs is effective
	\item Tying CRDTs to user interfaces via FRP reduces complexity by funneling local and remote changes through a consistent process and eliminates ad-hoc calls to APIs
	\item Our DFRP approach makes all collaboration real-time by default
	\item We can build up complex application states by rendering several related documents (directories, text notes, user profiles) each with their own context-specific rendering function for a particular CRDT
	\item It is important to have stable names referring to data that evolves over time, content hashing is not effective
	\item Automerge's performance is adequate for building real user applications
\end{itemize}

Networking
\begin{itemize}
    \item Peer-to-peer networking has three stages: content discovery, connection establishment, and synchronisation
    \item The most promising techniques for discovery are based on the distributed hash tables pioneered by BitTorrent, supplemented by mDNS locally
    \item Connection establishment is complicated by the widespread assumption of client-server architectures and widespread NAT routing
    \item Connectivity in public environments like cafes and corporate offices can be particularly challenging 
    \item The state of the art for NAT traversal in public environment is "hole punching", built on top of UDP. PushPin uses UTP, also from BitTorrent
    \item 
    \item Autonomic (self-validating data) is an valuable concept in a low-trust environment
	\item 
\end{itemize}

Future work
\begin{itemize}
    \item Migrating data between versions and preventing invalid document states remains a concerning problem
    \item Establishing direct peer-to-peer connectivity is problematic in several important environments
    \item Distributed Hash Table side channels are a concerning source of privacy leaks
    \item The PushPin implementation assumes all writes are accepted, an obvious area for additional research \& development
    \item New and old networking stacks have promise to improve connectivity, including BLE, WiFiDirect, and ultrasonic modems
    \item PushPin currently leaves almost all data un-encrypted, requiring trust in any peer that stores your data
    \item The need for rotation of public keys for data is a known and unexplored issue
    \item Some form of efficient document query or index documents are needed -- displaying a list of titles for documents requires fully loading all searchable documents
\end{itemize}

\begin{acks}
Thank you to Roshan Choxi, Ignatius Gilfedder, Mark McGranaghan, Jeff Peterson, and Matt Tognetti, who contributed to the development of PushPin.
The project was produced under the auspices of the Ink \& Switch research lab (\url{https://www.inkandswitch.com/}).
Martin Kleppmann is supported by a Leverhulme Trust Early Career Fellowship and by the Isaac Newton Trust.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}{}
\end{document}
